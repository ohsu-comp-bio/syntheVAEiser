{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc24d8e-9788-40cb-8c3d-328dfaff064b",
   "metadata": {},
   "source": [
    "#### Code corresponding to:\n",
    "\n",
    "Augmenting traditional ML with VAE-based gene expression sample generation for improved prediction of cancer molecular subtypes. Karlberg, B., et al. 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d35ea9-359f-4eea-9d69-2780a8635a46",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "conda create -n synth_cenv python=3.9 tensorflow=2.10 pandas numpy scikit-learn jupyterlab  \n",
    "conda activate synth_cenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8e2fe-b6bb-43df-9b68-ac24ebf6c6d4",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438a000f-c1f8-4d0a-b069-46f16f2f1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from random import sample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Layer\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82453c5d-431b-4f57-9238-671ca199002a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8875933-9229-4c64-a398-0bfdbb47c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent(x):\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim), mean=0., stddev=1.0 )\n",
    "    return mu + K.exp(sigma/2)*eps\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(\n",
    "            x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "    \n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607e1cda-5649-4d90-9b70-51c2d4f65055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hlvs(latent_object, synth_index_name):\n",
    "    print('Start synth sample gen from latent')\n",
    "    synth_in_count = 3 # <-- number of samples to recombine within each class\n",
    "    # synth_in_count = 2 # <-- comparison used in paper\n",
    "    synth_sub_len = 200\n",
    "\n",
    "    synth_ndx_strt = 0\n",
    "    synth_full_frame = pd.DataFrame(columns = latent_object.columns)\n",
    "\n",
    "    for subtype in sorted(latent_object.Labels.unique()):\n",
    "        print(subtype)\n",
    "        sub = latent_object[latent_object.Labels == subtype]\n",
    "        print(synth_sub_len)\n",
    "        synth_index = ['SYNTH-' + synth_index_name + '-' + jtem for jtem in [str(\n",
    "            item).zfill(5) for item in list(range(synth_ndx_strt,\n",
    "                                                  synth_sub_len + synth_ndx_strt))]]\n",
    "        synth_sub_frame = pd.DataFrame(index = synth_index)\n",
    "        synth_sub_frame.insert(0, 'Labels', sub.Labels[0])\n",
    "\n",
    "        synth_dict = {}\n",
    "        for synth_sample in synth_sub_frame.index:\n",
    "            input_sample_set = sub.sample(synth_in_count)\n",
    "            new_samp_vec = []\n",
    "            for col in input_sample_set.iloc[:, 1:]:\n",
    "                vals_inpt = input_sample_set.loc[:, col]\n",
    "                choosen_val = vals_inpt.sample(1)\n",
    "                new_samp_vec.append(choosen_val.values[0])\n",
    "\n",
    "            synth_dict[synth_sample] = new_samp_vec\n",
    "        synth_sub_frame = pd.concat([synth_sub_frame, pd.DataFrame(synth_dict).T], axis = 1)\n",
    "\n",
    "        synth_full_frame = pd.concat(\n",
    "            [synth_full_frame, synth_sub_frame], axis = 0) \n",
    "\n",
    "        synth_ndx_strt = synth_ndx_strt + synth_sub_len\n",
    "    print('Synthetic from latent done, '+str(synth_sub_len)+' samples generated for each subtype')\n",
    "    return(synth_full_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6562afaf-3bbc-402a-bb46-6be636c12a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnlvs(latent_object, synth_index_name):\n",
    "    synth_sub_len = 200\n",
    "    synth_ndx_strt = 0\n",
    "    synth_full_frame = pd.DataFrame(columns = latent_object.columns)\n",
    "    mu, sigma = 0, 0.1 # <-- increase sigma for more noise\n",
    "    # mu, sigma = 0, 0.2 # <-- comparison used in paper\n",
    "    for subtype in sorted(latent_object.Labels.unique()):\n",
    "        sub = latent_object[latent_object.Labels == subtype]\n",
    "\n",
    "        synth_index = [\n",
    "            'SYNTH-' + synth_index_name + '-' + jtem for jtem in [str(\n",
    "            item).zfill(5) for item in list(range(synth_ndx_strt,\n",
    "                                                  synth_sub_len + synth_ndx_strt))]]\n",
    "        synth_sub_frame = pd.DataFrame(index = synth_index)\n",
    "        synth_sub_frame.insert(0, 'Labels', sub.Labels[0])\n",
    "\n",
    "        synth_dict = {}\n",
    "        for synth_sample in synth_sub_frame.index:\n",
    "            sub_1 = sub.sample(1)\n",
    "\n",
    "            for col in sub_1.columns[1:]:\n",
    "                noise = np.random.normal(mu, sigma, sub_1[col].shape)\n",
    "                gaus = sub_1[col] + noise\n",
    "                if gaus[0] < 0:\n",
    "                    gaus[0] = 0\n",
    "                sub_1[col] = gaus\n",
    "            new_samp_vec = list(sub_1.iloc[0, 1:])\n",
    "            synth_dict[synth_sample] = new_samp_vec\n",
    "\n",
    "        synth_sub_frame = pd.concat(\n",
    "            [synth_sub_frame, pd.DataFrame(synth_dict).T], axis = 1)\n",
    "\n",
    "        synth_full_frame = pd.concat(\n",
    "            [synth_full_frame, synth_sub_frame], axis = 0) \n",
    "\n",
    "        synth_ndx_strt = synth_ndx_strt + synth_sub_len\n",
    "    return(synth_full_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63ce3776-f09f-4d1d-b142-38f148761fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndm_forest(trn, val):\n",
    "\n",
    "    val_acrcy_lst = []\n",
    "    val_running_average = []\n",
    "\n",
    "    X_trn = trn.iloc[:, 1:]\n",
    "    y_trn = trn.iloc[:, 0]\n",
    "    X_val = val.iloc[:, 1:]\n",
    "    y_val = val.iloc[:, 0]\n",
    "\n",
    "    for r in list(range(0, 10)):\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_trn, y_trn)\n",
    "        val_raw_acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        val_acrcy_lst.append(val_raw_acc)\n",
    "        val_running_average.append(mean(val_acrcy_lst))\n",
    "    return(val_running_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d55ce-5def-4fb4-b468-572b02812ef4",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac186723-9f6a-4c5f-8490-98e1c6786a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC_gene_xpr.tsv       KIRP_gene_xpr.tsv      SARC_gene_xpr.tsv\n",
      "BLCA_gene_xpr.tsv      LIHCCHOL_gene_xpr.tsv  SKCM_gene_xpr.tsv\n",
      "BRCA_gene_xpr.tsv      LUAD_gene_xpr.tsv      TGCT_gene_xpr.tsv\n",
      "CESC_gene_xpr.tsv      LUSC_gene_xpr.tsv      THCA_gene_xpr.tsv\n",
      "COADREAD_gene_xpr.tsv  MESO_gene_xpr.tsv      THYM_gene_xpr.tsv\n",
      "ESCC_gene_xpr.tsv      OV_gene_xpr.tsv        UCEC_gene_xpr.tsv\n",
      "GEA_gene_xpr.tsv       PAAD_gene_xpr.tsv      UVM_gene_xpr.tsv\n",
      "HNSC_gene_xpr.tsv      PCPG_gene_xpr.tsv\n",
      "KIRCKICH_gene_xpr.tsv  PRAD_gene_xpr.tsv\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91ad0f80-5cef-40a1-8f92-98bb861b1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir i_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b587f3-860c-4216-91ec-c1a6052c2672",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b1084-49f9-456a-bb07-8fa3e6464b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting sample generation and validation')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "v = 'synth_v0/'\n",
    "feature_files = 'data/*.tsv'\n",
    "cohort_n_ndx = 0 # <--- toggle test-cohort here, 0 = ACC_gene_xpr.tsv\n",
    "trn_size = 40\n",
    "pre_train_epochs = 4\n",
    "\n",
    "inpt_val = pd.DataFrame()\n",
    "dec_val = pd.DataFrame()\n",
    "synth_lat_val = pd.DataFrame()\n",
    "blend_val = pd.DataFrame()\n",
    "\n",
    "file_paths = sorted(\n",
    "    glob.glob(feature_files))\n",
    "print('Total cohorts n = ', len(file_paths))\n",
    "\n",
    "fine_tune_file = pd.read_csv(\n",
    "    file_paths[cohort_n_ndx],\n",
    "    sep = '\\t', index_col = 0)\n",
    "print(fine_tune_file.index.name)\n",
    "\n",
    "out_dirs = ['/decoded_objs/',\n",
    "            '/latent_objs/',\n",
    "            '/loss_plots/',\n",
    "            '/take-off_points/',\n",
    "            '/synthetic_sample_sets/',\n",
    "            '/rfe_out/']\n",
    "\n",
    "for out_dir in out_dirs:\n",
    "    auto_path_name = 'i_o/'+v+fine_tune_file.index.name+out_dir\n",
    "    print(auto_path_name)\n",
    "    os.makedirs(os.path.dirname(auto_path_name), exist_ok = True)\n",
    "\n",
    "pre_train_file = pd.DataFrame()\n",
    "\n",
    "file_paths.remove(\n",
    "    file_paths[cohort_n_ndx])\n",
    "\n",
    "print('Pre-train on cohorts n = ', len(file_paths))\n",
    "for path in file_paths:\n",
    "    file = pd.read_csv(path, sep = '\\t', index_col = 0)\n",
    "    pre_train_file = pd.concat( [pre_train_file, file] , axis = 0)\n",
    "\n",
    "# Each validation split constitutes an experimental replicate\n",
    "vs_list = ['vs01@','vs02@','vs03@','vs04@','vs05@','vs06@','vs07@','vs08@','vs09@','vs10@',\n",
    "           'vs11@','vs12@','vs13@','vs14@','vs15@','vs16@','vs17@','vs18@','vs19@','vs20@',\n",
    "           'vs21@','vs22@','vs23@','vs24@','vs25@']\n",
    "\n",
    "pre_train_loss_dict = {}\n",
    "fine_tune_loss_dict = {}\n",
    "for validation_split in vs_list:\n",
    "    print(validation_split)\n",
    "    val_split = validation_split + str(trn_size)\n",
    "    trn = fine_tune_file.sample(trn_size)\n",
    "    while_loop_val = 0\n",
    "    while trn.Labels.value_counts().min() < 3:\n",
    "        trn = fine_tune_file.sample(trn_size)\n",
    "        while_loop_val += 1\n",
    "        if while_loop_val == 50:\n",
    "            break\n",
    "    print('While undersampled loops: ', while_loop_val)\n",
    "    if trn.Labels.value_counts().min() < 3:\n",
    "        continue\n",
    "    val = fine_tune_file.loc[fine_tune_file[~fine_tune_file.index.isin(\n",
    "        trn.index)].index, :]    \n",
    "    inpt_val.insert(0, val_split, rndm_forest(trn, val))\n",
    "    \n",
    "    # Pre-train\n",
    "    train_file = pre_train_file\n",
    "    fit_on = str(len(pre_train_file))\n",
    "    pre_trn = 'NONE'\n",
    "    feature_set = feature_files.split('/')[-2]\n",
    "\n",
    "    fine_tune_epochs = 'NA'\n",
    "    features = train_file.columns[1:]\n",
    "\n",
    "    original_dim = len(features)\n",
    "    feature_dim = len(features)\n",
    "    latent_dim = 250\n",
    "    batch_size = 50\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(feature_dim,))\n",
    "    z_mean_dense_linear = layers.Dense(\n",
    "        latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_1\")(encoder_inputs)\n",
    "    z_mean_dense_batchnorm = layers.BatchNormalization()(z_mean_dense_linear)\n",
    "    z_mean_encoded = layers.Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "    z_log_var_dense_linear = layers.Dense(\n",
    "        latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_2\")(encoder_inputs)\n",
    "    z_log_var_dense_batchnorm = layers.BatchNormalization()(z_log_var_dense_linear)\n",
    "    z_log_var_encoded = layers.Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    latent_space = layers.Lambda(\n",
    "        compute_latent, output_shape=(\n",
    "            latent_dim,), name=\"latent_space\")([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    decoder_to_reconstruct = layers.Dense(\n",
    "        feature_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "    decoder_outputs = decoder_to_reconstruct(latent_space)\n",
    "\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    kappa = 1\n",
    "    beta = K.variable(0)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    vae_layer = CustomVariationalLayer()([encoder_inputs, decoder_outputs])\n",
    "    vae = Model(encoder_inputs, vae_layer)\n",
    "    vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "    pre_train_epochs = pre_train_epochs\n",
    "\n",
    "    fit_start = time.time()\n",
    "    history = vae.fit(train_file.iloc[:, 1:],\n",
    "                epochs=pre_train_epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[WarmUpCallback(beta, kappa)],\n",
    "                      verbose=0)\n",
    "    pre_train_loss_dict[validation_split] = history.history['loss']\n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    # Fine-tune\n",
    "    train_file = trn\n",
    "    pre_trn = 'TCGA_n='+fit_on\n",
    "    fit_on = trn.index.name\n",
    "    \n",
    "    fine_tune_epochs = 150\n",
    "    \n",
    "    batch_size = 10\n",
    "    history = vae.fit(train_file.iloc[:, 1:],\n",
    "                epochs = fine_tune_epochs, batch_size=batch_size, shuffle=True,\n",
    "                callbacks = [WarmUpCallback(beta, kappa)], verbose=0)\n",
    "    fine_tune_loss_dict[validation_split] = history.history['loss']\n",
    "    \n",
    "    encoder = Model(encoder_inputs, z_mean_encoded)\n",
    "    decoder_input = keras.Input(shape = (latent_dim, ))\n",
    "    _x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "    decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    y_df = train_file.Labels\n",
    "    decoded = pd.DataFrame(decoder.predict(encoder.predict(train_file.iloc[:, 1:])),\n",
    "                           index = train_file.index, columns = train_file.iloc[:, 1:].columns)\n",
    "    \n",
    "    latent_object = pd.DataFrame(encoder.predict(train_file.iloc[:, 1:]),\n",
    "                 index=train_file.index)\n",
    "    latent_object.index.name = trn.index.name\n",
    "    latent_object = pd.concat( [pd.DataFrame(y_df), latent_object] , axis =1)\n",
    "    \n",
    "    decoded_labeled = pd.concat( [pd.DataFrame(y_df), decoded] , axis =1)\n",
    "    decoded_labeled.to_csv(\n",
    "        'i_o/'+v+'/'+fine_tune_file.index.name+'/decoded_objs/fit.'+\n",
    "        fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "        '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "        '_decoded_obj_latent_dim.'+str(latent_dim)+\n",
    "        '_'+feature_set+'_'+val_split+'.tsv', sep = '\\t')\n",
    "    \n",
    "    dec_val.insert(0, val_split, rndm_forest(decoded_labeled, val))\n",
    "\n",
    "    synth_full_frame = hlvs(latent_object, fine_tune_file.index.name)\n",
    "    # synth_full_frame = rnlvs(latent_object, fine_tune_file.index.name)\n",
    "    \n",
    "    synth_lat_dec = pd.concat([synth_full_frame.iloc[:, 0],\n",
    "                       pd.DataFrame(decoder.predict(synth_full_frame.iloc[:, 1:]),\n",
    "                                    index = synth_full_frame.index)], axis = 1)\n",
    "    synth_lat_dec.columns = trn.columns\n",
    "    synth_lat_dec.to_csv(\n",
    "        'i_o/'+v+fine_tune_file.index.name+'/synthetic_sample_sets/fit.'+fit_on+\n",
    "        '_epochs.'+str(fine_tune_epochs)+'_pre_trained_on.'+pre_trn+'_epochs.'+\n",
    "        str(pre_train_epochs)+'_synthetic_sample_set_latent_dim.'+str(latent_dim)+\n",
    "        '_'+feature_set+'_'+val_split+'.tsv', sep = '\\t')\n",
    "    \n",
    "    synth_lat_val.insert(0, val_split, rndm_forest(synth_lat_dec, val))\n",
    "    \n",
    "    blend = pd.concat([trn, synth_lat_dec], axis = 0)\n",
    "    blend_val.insert(0, val_split, rndm_forest(blend, val))\n",
    "    \n",
    "    blend_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                     '_blend_val.tsv', sep = '\\t')\n",
    "    \n",
    "    inpt_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                    '_input_val.tsv', sep = '\\t')\n",
    "    \n",
    "    dec_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                   '_decoded_val.tsv', sep = '\\t')\n",
    "    \n",
    "    synth_lat_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                   '_synth_lat_val.tsv', sep = '\\t')\n",
    "    # End experimental replicate loop\n",
    "pre_train_loss_frame = pd.DataFrame(pre_train_loss_dict)\n",
    "fine_tune_loss_frame = pd.DataFrame(fine_tune_loss_dict)\n",
    "\n",
    "pre_train_loss_frame.to_csv(\n",
    "    'i_o/'+v+'/'+fine_tune_file.index.name+'/loss_plots/fit.'+\n",
    "    fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "    '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "    '_pre_train_loss_vals_latent_dim.'+str(latent_dim)+\n",
    "    '_'+feature_set+'.tsv', sep = '\\t')\n",
    "\n",
    "fine_tune_loss_frame.to_csv(\n",
    "    'i_o/'+v+'/'+fine_tune_file.index.name+'/loss_plots/fit.'+\n",
    "    fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "    '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "    '_fine_tune_loss_vals_latent_dim.'+str(latent_dim)+\n",
    "    '_'+feature_set+'.tsv', sep = '\\t')\n",
    "\n",
    "print('main done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
